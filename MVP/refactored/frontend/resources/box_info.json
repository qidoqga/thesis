{
  "transformer builder": "This box needs to be the first box in the diagram when building a transformer architecture, and it needs to have at least two inputs, but may have up to seven inputs:\n\nInput 1 (required): Model Dimension (no default)\nInput 2 (required): Vocabulary Size (no default)\nInput 3: Number of Heads (default = 8)\nInput 4: Activation (default = 'relu')\nInput 5: Dropout (default = 0.1)\nInput 6: Maximum Length (default = 500)\nInput 7: Feedforward Dimension (default = 2048)",
  "embedding layer": "Gets model dimension and vocabulary size from transformer builder box.\n\nThis box makes sure that raw token IDs are transformed into the proper vector format before any computation takes place.",
  "positional encoding layer": "Gets model dimension, dropout, and maximum length from transformer builder box.\n\nThis box ensures that sequential position information is added before any attention or feedforward operations take place.",
  "encoder builder": "This box should be used at the start of an encoder block so that the following layers would be added in the correct order.",
  "normalization layer": "This box can be used whenever the previous layer’s outputs need normalization.",
  "self attention layer": "Gets model dimension, number of heads, and dropout from transformer builder box.\n\nThis self-attention layer box lets each token in the sequence look at every other token and decide how much to focus on each token.",
  "build encoder self attention": "This box is used in the encoder block to create the self-attention block.",
  "feedforward builder": "This box is used to mark the start of the feedforward block, and the next layers are added to this block.",
  "linear layer": "Gets feedforward dimension from transformer builder box.\n\nThe linear layer box works like a regular feedforward dense layer.",
  "activation layer": "Gets activation from transformer builder box.\n\nThe activation layer box introduces non-linearity into the model computations.",
  "dropout layer": "Gets dropout from transformer builder box.\n\nThe dropout layer box prevents the model from relying too heavily on any activation, reducing overfitting.",
  "build encoder": "Gets model dimension from transformer builder box.\n\nThis box marks the end of the encoder block layers.",
  "decoder builder": "This box should be used at the start of a decoder block so that the following layers would be added in the correct order.",
  "build decoder self attention": "This box builds the self-attention block for the decoder block, and adds it to the decoder block.",
  "cross attention builder": "This box is used to start the cross attention block.",
  "cross attention layer": "Gets model dimension, number of heads, and dropout from transformer builder box.\n\nThe cross-attention layer box lets one sequence attend to a different sequence, typically the encoder’s outputs.",
  "build cross attention": "This box is used to mark the end of the cross-attention block in the decoder.",
  "build decoder": "Gets model dimension from transformer builder box.\n\nThis box marks the end of the decoder block layers.",
  "output layer": "Gets model dimension and vocabulary size from transformer builder box.\n\nThe output layer box projects the final hidden vectors into the desired output space.",
  "build transformer": "This box should always be the last box of the transformer model in the diagram, because this builds the actual model that is created in the diagram.",

  "ffn input layer": "This box needs one input, which should be the Number of Input Features.\n\nThis box defines the dimensionality of the data entering the network. Every model must begin with this input layer box so that other layers know how many features to expect.",
  "ffn hidden layer": "This box adds a fully connected layer to the model that transforms its inputs into a higher or a lower dimensional space and then applies a non-linearity.\n\nSet parameters:\n\nNeurons: {neurons}\nActivation: {activation}",
  "ffn batch norm": "This batch normalization box normalizes and rescales the activations on each batch to stabilize and accelerate the training process.",
  "ffn dropout": "Dropout box randomly zeroes a fraction of the hidden activations at training time to reduce overfitting and improve generalization.\n\nSet parameters:\n\nDropout: {dropout}",
  "ffn output layer": "This layer produces the final predictions or scores.\n\nSet parameters:\n\nNumber of Outputs: {outputs}\nActivation: {activation}",

  "cnn input layer": "This box needs one input, which should be one tuple in this order: number of channels, image height, image width.\n\nThis box establishes the shape of the incoming image tensor so that subsequent layers can be configured correctly",
  "cnn conv layer": "This layer applies a bank of learnable filters to the input, producing feature maps that highlight local patterns, and activation adds non-linearity right after.\n\nSet parameters:\n\nNumber of Output Channels: {out_channels}\nKernel Size: {kernel_size}\nStride: {stride}\nPadding: {padding}\nActivation: {activation}",
  "cnn pool": "This box reduces spatial dimensions and elevates translational invariance by pooling each region of feature maps.\n\nSet parameters:\n\nKernel Size: {kernel_size}\nStride: {stride}\nPool Type: {pool_type}",
  "cnn dropout2d": "This box randomly zeros whole feature maps (channels) during training to avoid coadaptations and increase robustness.\n\nSet parameters:\n\nDropout: {dropout}",
  "cnn flatten": "Flattening box converts the multidimensional tensor of feature maps into a single vector.",
  "cnn dense layer": "This box acts as a standard fully connected layer.\n\nSet parameters:\n\nNeurons: {neurons}\nActivation: {activation}",
  "cnn dropout": "This 1D dropout box applies regular dropout in the fully connected portion fo the network.\n\nSet parameters:\n\nDropout: {dropout}",
  "cnn output layer": "This box outputs the final class scores or regression outputs and concludes the convolutional pipeline.\n\nSet parameters:\n\nNumber of Outputs: {neurons}\nActivation: {activation}",

  "rnn input layer": "This box needs one input, which should be feature dimension.\n\nThis box establishes how the data will flow into the recurrent stack.\n\nSet parameters:\n\nBatch First: {batch_first}\nSeq To Seq: {seq_to_seq}",
  "rnn lstm layer": "This box can be used to add Long Short-Term Memory layers to the model, which maintains separate cell and hidden states to capture longer temporal dependencies.\n\nSet parameters:\n\nNeurons: {neurons}\nNumber of Layers: {num_layers}\nBidirectional: {bidirectional}\nInternal Dropout: {dropout}",
  "rnn gru layer": "This box can be used to add Gated Recurrent Unit layers, which are a simpler alternative to LSTM layers.\n\nSet parameters:\n\nNeurons: {neurons}\nNumber of Layers: {num_layers}\nBidirectional: {bidirectional}\nInternal Dropout: {dropout}",
  "rnn simple layer": " Simple RNN layer box adds vanilla RNN layer to the model. This layer box is useful for basic sequence-to-sequence tasks or lightweight time series models.\n\nSet parameters:\n\nNeurons: {neurons}\nNumber of Layers: {num_layers}\nNon-Linearity: {non_linearity}\nBidirectional: {bidirectional}\nInternal Dropout: {dropout}",
  "rnn dropout": "This box applies dropout directly to the RNN output activations, after any internal recurrence, so that units are disabled randomly during training.\n\nSet parameters:\n\nDropout: {dropout}",
  "rnn output layer": "This output box projects the final hidden representation, or the last time-step in sequence to sequence mode, to the desired output space\n\nSet parameters:\n\nNumber of Outputs: {neurons}\nActivation: {activation}"
}